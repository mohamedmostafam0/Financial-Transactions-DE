# Financial Transactions Data Engineering Pipeline

> **âš ï¸ WORK IN PROGRESS**: This project is currently under active development and is not yet complete. Features and documentation may change significantly.

A comprehensive data engineering pipeline for processing financial transactions, with enriched user and merchant data, fraud detection capabilities, and economic context integration.

## Project Overview

This project simulates a real-world financial transactions processing system using modern data engineering practices. It follows an ELT (Extract, Load, Transform) approach, leveraging cloud technologies and event-driven architecture.

### Key Features

- **Rich Data Generation**: Realistic user, merchant, and transaction data with detailed attributes
- **Event Streaming**: Kafka-based event processing pipeline
- **Cloud Integration**: Google Cloud Storage and BigQuery for data warehousing
- **Economic Context**: Integration with FRED economic indicators API
- **Data Quality**: Schema validation and error handling
- **Flexible Storage**: PostgreSQL for operational data, BigQuery for analytics

## ğŸ“Š Data Models

### User Data
- Demographics (age, gender, occupation)
- Location information (country, city, coordinates)
- Financial attributes (credit score, risk score, income bracket)
- Preferences and spending patterns

### Merchant Data
- Business details (category, founding date, size)
- Location information
- Accepted payment methods
- Risk profile and fraud history

### Transaction Data
- Core transaction details (amount, currency, timestamp)
- Location context (online vs. in-person)
- Payment method details
- Transaction type and status

### Economic Indicators
- Unemployment rates
- Consumer Price Index
- GDP growth
- Federal funds rate
- Consumer sentiment

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Producers  â”‚â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â–¶â”‚   Consumers   â”‚â”€â”€â”€â–¶â”‚ Google Cloudâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                                       â–²
      â”‚                                                       â”‚
      â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ PostgreSQL  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

1. **Producers**:
   - User data generator
   - Merchant data generator
   - Transaction simulator
   - Economic indicators service

2. **Consumers**:
   - Fraud detection service
   - Data loading to BigQuery
   - Economic indicators consumer

3. **Storage**:
   - PostgreSQL for operational data
   - Google Cloud Storage for data lake
   - BigQuery for data warehouse

## ğŸ› ï¸ Technologies Used

- **Python**: Core application logic
- **Apache Kafka**: Event streaming
- **PostgreSQL**: Operational database
- **Google Cloud Storage**: Data lake
- **Google BigQuery**: Data warehouse
- **Avro**: Schema definition and serialization
- **FRED API**: Economic data integration
- **Faker**: Synthetic data generation

## ğŸš¦ Getting Started

### Prerequisites

- Python 3.8+
- Docker and Docker Compose
- Google Cloud account
- Confluent Kafka account (or local Kafka setup)
- PostgreSQL

### Environment Setup

1. Clone the repository
2. Create a `.env` file based on `.env.example`
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Running the Pipeline

1. Start the PostgreSQL database:
   ```bash
   docker-compose up -d postgres
   ```

2. Initialize the database schema:
   ```bash
   python -m src.utils.postgres_db
   ```

3. Start the producers:
   ```bash
   python -m src.producers.user_producer
   python -m src.producers.merchant_producer
   python -m src.producers.transaction_producer
   python -m src.services.economic_indicators_service
   ```

4. Start the consumers:
   ```bash
   python -m src.consumers.economic_indicators_consumer
   ```

## ğŸ“ Project Structure

```
Financial-Transactions-DE/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producers/            # Data generation
â”‚   â”œâ”€â”€ consumers/            # Data processing
â”‚   â”œâ”€â”€ services/             # External services integration
â”‚   â”œâ”€â”€ utils/                # Shared utilities
â”‚   â”œâ”€â”€ schemas/              # Avro schemas
â”‚   â””â”€â”€ etl/                  # ELT transformations
â”œâ”€â”€ DWH/                      # Data warehouse artifacts
â”œâ”€â”€ airflow/                  # Airflow configuration
â”œâ”€â”€ config/                   # Configuration files
â”œâ”€â”€ dags/                     # Airflow DAGs
â”œâ”€â”€ data_contracts/           # Data contracts and specifications
â”œâ”€â”€ jars/                     # Java dependencies
â”œâ”€â”€ docker-compose.yml        # Docker configuration
â””â”€â”€ .env                      # Environment variables
```

## Acknowledgements
- Federal Reserve Economic Data (FRED) for economic indicators
- Faker library for synthetic data generation
- Confluent Kafka for event streaming capabilities
- Google Cloud for data warehousing solutions
